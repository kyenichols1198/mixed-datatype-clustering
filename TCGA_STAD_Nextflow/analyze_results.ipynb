{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Defining a function to extract parameters from the filename\n",
    "def extract_params(filename):\n",
    "    name_without_extension = os.path.splitext(filename)[0]\n",
    "    params_str = name_without_extension.split(\"_\")\n",
    "    params_dict = {}\n",
    "    for i in range(0, len(params_str), 2):\n",
    "        params_dict[params_str[i]] = params_str[i+1]\n",
    "    return params_dict\n",
    "\n",
    "# Path to the directory with the files\n",
    "path = 'results/'\n",
    "\n",
    "# Lists to store dataframes along with their parameters\n",
    "labels_data = []\n",
    "scores_data = []\n",
    "weights_data = []\n",
    "distances_data = []\n",
    "feature_order_data = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the data\n",
    "        df = pd.read_csv(path + filename)\n",
    "        \n",
    "        # Extract parameters from the filename\n",
    "        params = extract_params(filename)\n",
    "        \n",
    "        # Depending on the type of data, append it to the appropriate list\n",
    "        if 'labels' in filename:\n",
    "            labels_data.append((df, params))\n",
    "        elif 'scores' in filename:\n",
    "            scores_data.append((df, params))\n",
    "        elif 'weights' in filename:\n",
    "            weights_data.append((df, params))\n",
    "        elif 'distance_matrix' in filename:\n",
    "            distances_data.append((df, params))\n",
    "        elif \"feature_orders\" in filename:\n",
    "            feature_order_data.append((df, params))\n",
    "            \n",
    "# Now we have four lists - each containing tuples of dataframes and their parameters\n",
    "# For each type of data (labels, scores, weights, distances), we have a list of these tuples\n",
    "# Each tuple has a dataframe and a dictionary with parameters extracted from the filename\n",
    "\n",
    "# Let's check the first few entries of each dataframe to ensure that the data has been loaded correctly\n",
    "# labels_data[0][0].head(), scores_data[0][0].head(), weights_data[0][0].head(), distances_data[0][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a function to combine files of the same type into a single file\n",
    "\n",
    "def combine_files(file_data, output_file):\n",
    "    # Initialize an empty dataframe to store the combined data\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through all dataframes and their parameters\n",
    "    for df, params in file_data:\n",
    "        # Rename the column with the data to reflect the parameters\n",
    "        column_name = '_'.join([f\"{k}={v}\" for k, v in params.items()])\n",
    "        df = df.rename(columns={df.columns[0]: column_name})\n",
    "        # If the combined dataframe is empty, replace it with the current dataframe\n",
    "        if combined_df.empty:\n",
    "            combined_df = df\n",
    "        else:\n",
    "            # Otherwise, merge the current dataframe with the combined dataframe\n",
    "            combined_df = pd.concat([combined_df, df], axis=1)\n",
    "    \n",
    "    # Write the combined dataframe to a CSV file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    return combined_df\n",
    "\n",
    "# Now, let's use this function to combine all the cluster score files into a single file\n",
    "combined_scores = combine_files(scores_data, 'combined_results/combined_scores.csv')\n",
    "\n",
    "# And all the cluster weight files into a single file\n",
    "combined_weights = combine_files(weights_data, 'combined_results/combined_weights.csv')\n",
    "\n",
    "# And all the cluster weight files into a single file\n",
    "combined_labels = combine_files(labels_data, 'combined_results/combined_labels.csv')\n",
    "\n",
    "# And all the feature orders into a single file\n",
    "combined_orders = combine_files(feature_order_data, 'combined_results/feature_orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got the column named from the data originally but I'll just hard-code them here so\n",
    "# I don't need to upload the data\n",
    "# tabular_data = pd.read_csv(\"work/91/0e2c6a8e9ff6d453846d2dfbf9d928/maf_files/binary_numerical_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of getting them from the data, let's hard-code the original feature names and order\n",
    "#feature_cols = list(tabular_data.columns.values)\n",
    "feature_cols = ['Unnamed: 0', 'age_at_diagnosis', 'year_of_diagnosis', \n",
    "                'Frame_Shift_Del', 'Frame_Shift_Ins', 'In_Frame_Del', \n",
    "                'In_Frame_Ins', 'Missense_Mutation', 'Nonsense_Mutation', \n",
    "                'Nonstop_Mutation', 'Splice_Site', 'Translation_Start_Site', \n",
    "                'MedianAbsoluteDeviation', 'MATH', 'prior_malignancy', \n",
    "                'prior_malignancy.1', 'gender', 'tissue_or_organ_of_origin', \n",
    "                'primary_diagnosis', 'ajcc_staging_system_edition', 'ajcc_pathologic_t', \n",
    "                'morphology', 'ajcc_pathologic_n', 'ajcc_pathologic_m', 'icd_10_code', \n",
    "                'site_of_resection_or_biopsy', 'race', 'ethnicity', \n",
    "                'treatments_radiation_treatment_or_therapy', 'vital_status']\n",
    "\n",
    "feature_indexes = list(combined_orders.iloc[:,0].apply(int))\n",
    "sorted_cols = sorted(feature_cols, key=lambda x: feature_indexes.index(feature_cols.index(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Weight Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "weights = combined_weights\n",
    "weights.index = sorted_cols\n",
    "\n",
    "# Melt the DataFrame to a long format\n",
    "melted_weights = weights.reset_index().melt(id_vars='index', var_name='parameters', value_name='weight')\n",
    "\n",
    "# Extract K and distance metric from parameters\n",
    "melted_weights['K'] = melted_weights['parameters'].apply(lambda x: x.split('_')[0].split('=')[3])\n",
    "melted_weights['distance_metric'] = melted_weights['parameters'].apply(lambda x: x.split('_')[1].split('=')[1])\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=melted_weights, x='index', y='weight')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of weights for each feature grouped by distance metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "distance_metrics = melted_weights['distance_metric'].unique()\n",
    "\n",
    "# Iterate over unique distance metrics\n",
    "for metric in distance_metrics:\n",
    "    # Filter data for the specific distance metric\n",
    "    subset = melted_weights[melted_weights['distance_metric'] == metric]\n",
    "    \n",
    "    # Create a new figure for each distance metric\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Create a boxplot for the subset\n",
    "    sns.boxplot(x='index', y='weight', hue='K', data=subset)\n",
    "    plt.title(f'Boxplot for {metric} distance metric')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # Display the plot for the current distance metric\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huang distance metric just 0's everything except the age of the patient at diagnosis (this is why changing clusters or regularization really doesn't change anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the data to long format\n",
    "data = combined_scores\n",
    "data_long = data.melt(var_name='parameters', value_name='scores')\n",
    "\n",
    "# Split the parameters into separate columns\n",
    "data_long[['P', 'K', 'dist', 'S']] = data_long['parameters'].str.extract(r'P=(\\d+)=K=(\\d+)_dist=(\\w+)=S=(\\d\\.\\d)', expand=True)\n",
    "\n",
    "# Convert N and K to integers and S to float for proper sorting in visualizations\n",
    "data_long['P'] = data_long['P'].astype(int)\n",
    "data_long['K'] = data_long['K'].astype(int)\n",
    "data_long['S'] = data_long['S'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the overall aesthetic.\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create line plots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 15))\n",
    "\n",
    "# Plot for K\n",
    "sns.lineplot(x='K', y='scores', hue='dist', data=data_long, ax=axs[0])\n",
    "axs[0].set_title('Scores by Number of Clusters (K)')\n",
    "\n",
    "# Plot for S\n",
    "sns.lineplot(x='S', y='scores', hue='dist', data=data_long, ax=axs[1])\n",
    "axs[1].set_title('Scores by Scaling Factor (S)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the cluster assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('combined_results/combined_labels.csv')\n",
    "\n",
    "# Extracting K, dist, and S from column names\n",
    "def extract_params(col):\n",
    "    match = re.match(r\"P=30=K=(\\d+)_dist=([\\w]+)=S=([\\d.]+)_cluster=labels\", col)\n",
    "    if match:\n",
    "        K = int(match.group(1))\n",
    "        dist = match.group(2)\n",
    "        S = float(match.group(3))\n",
    "        return K, dist, S\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Add K, dist, and S to the dataframe\n",
    "params = pd.DataFrame([extract_params(col) for col in data.columns], columns=['K', 'dist', 'S'])\n",
    "params['label'] = data.columns\n",
    "data_melted = pd.melt(data.reset_index(), id_vars='index', value_vars=data.columns)\n",
    "data_melted.columns = ['index', 'label', 'cluster']\n",
    "data_melted = data_melted.merge(params, on='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_K = data_melted['K'].unique()\n",
    "num_K = len(unique_K)\n",
    "# Creating the plots with bars side-by-side\n",
    "\n",
    "fig, axs = plt.subplots(num_K, figsize=(15, 6*num_K))\n",
    "\n",
    "for i, K in enumerate(unique_K):\n",
    "    data_subset = data_melted[data_melted['K'] == K]\n",
    "    sns.histplot(data=data_subset, x=\"cluster\", hue=\"dist\", multiple=\"dodge\", shrink=.8, ax=axs[i])\n",
    "    axs[i].set_title(f'Number of Points in Each Cluster for K={K}')\n",
    "    axs[i].set_xlabel('Cluster')\n",
    "    axs[i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the plots again, this time coloring by S\n",
    "\n",
    "fig, axs = plt.subplots(num_K, figsize=(15, 6*num_K))\n",
    "\n",
    "for i, K in enumerate(unique_K):\n",
    "    data_subset = data_melted[data_melted['K'] == K]\n",
    "    sns.histplot(data=data_subset, x=\"cluster\", hue=\"S\", multiple=\"dodge\", shrink=.8, palette=\"viridis\", ax=axs[i])\n",
    "    axs[i].set_title(f'Number of Points in Each Cluster for K={K}')\n",
    "    axs[i].set_xlabel('Cluster')\n",
    "    axs[i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data and understand its structure.\n",
    "weights = pd.read_csv('combined_results/combined_weights.csv')\n",
    "# Let's create a new dataframe where for every column we list the indexes of the most important three features\n",
    "\n",
    "def top_three_features(col):\n",
    "    top_three = col.nlargest(3)\n",
    "    return pd.Series(top_three.index)\n",
    "\n",
    "top_features = weights.apply(top_three_features)\n",
    "top_features.columns = weights.columns\n",
    "top_features.transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
